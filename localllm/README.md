## LOCAL LLM
DeepSeek Chat VS Code Extension
This Visual Studio Code extension allows you to chat with the DeepSeek model locally. The extension requires the Ollama framework and the DeepSeek model (deepseek-r1:1.5b) to be installed on your PC.

Features
Chat with the DeepSeek model directly within VS Code.
Seamless integration with the Ollama framework.
Local execution for privacy and speed.
Requirements
Visual Studio Code
Ollama framework installed on your PC
DeepSeek model (deepseek-r1:1.5b) installed on your PC
Installation
Install Ollama Framework: Follow the instructions on the Ollama website to install the Ollama framework on your PC.

Install DeepSeek Model: Ensure that the DeepSeek model (deepseek-r1:1.5b) is installed. You can download and install it from the DeepSeek website.

Install the Extension:

Open Visual Studio Code.
Go to the Extensions view by clicking on the Extensions icon in the Activity Bar on the side of the window or by pressing Ctrl+Shift+X.
Search for "DeepSeek Chat".
Click "Install" to install the extension.
Usage
Open the Command Palette:

Press Ctrl+Shift+P to open the Command Palette.
Start Chatting:

Type startllm: Start Chat and press Enter.
A chat window will open where you can start interacting with the DeepSeek model.
Configuration
You can configure the extension settings by going to File > Preferences > Settings and searching for "DeepSeek Chat". Here you can set various options such as:

Model Path: Path to the DeepSeek model.
Ollama Path: Path to the Ollama framework executable.